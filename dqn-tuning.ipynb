{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy, MlpLstmPolicy, MlpLnLstmPolicy, CnnPolicy, CnnLstmPolicy, CnnLnLstmPolicy\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import DQN\n",
    "\n",
    "from gym.utils import seeding\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "import numpy as np\n",
    "\n",
    "from ads_utils import load_data, plot, Environment\n",
    "from tqdm import tqdm\n",
    "\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  \"name\": \"dqn sweep\",\n",
    "  \"method\": \"grid\",\n",
    "  \"parameters\": {\n",
    "      \"learning_rate\": {\n",
    "            \"values\": [0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00001]\n",
    "        },\n",
    "      \"gamma\": {\n",
    "            \"values\": [0.5, 0.9, 0.99]\n",
    "        },\n",
    "      \"batch_size\": {\n",
    "          \"values\": [10, 50, 70, 100]  \n",
    "        },\n",
    "      \"n_ticks\": {\n",
    "          \"values\": [1, 5, 20, 50, 150, 200]\n",
    "      }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: v1fpgwgw\n",
      "Sweep URL: https://wandb.ai/ads/dqn-sweep-all-params-val-data/sweeps/v1fpgwgw\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, entity=\"ads\", project=\"dqn-sweep-all-params-val-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = load_data([i for i in range(1, 12 + 1)])['close'].to_list()\n",
    "val_data = load_data([i for i in range(13, 18 + 1)])['close'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_env(ticks):\n",
    "    INITIAL_BALANCE = 10_000\n",
    "    # sample training data\n",
    "#     start = randint(0, len(training_data) - 10000 - 1) \n",
    "#     sample = training_data[start: start + 10000]\n",
    "    return Environment(training_data, balance=INITIAL_BALANCE, past_ticks=ticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_env(ticks):\n",
    "    INITIAL_BALANCE = 10_000\n",
    "    val_env = Environment(val_data, balance=INITIAL_BALANCE, past_ticks=ticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    run = wandb.init()\n",
    "    print(\"config:\", dict(run.config))\n",
    "    \n",
    "    N_EPOCH = 200\n",
    "    env = create_training_env(run.config.n_ticks)\n",
    "    env.reset()\n",
    "    model = DQN('MlpPolicy', env, verbose=0, \n",
    "                learning_rate=run.config.learning_rate, \n",
    "                gamma=run.config.gamma,\n",
    "                batch_size=run.config.batch_size)\n",
    "    \n",
    "    for i in range(N_EPOCH):  \n",
    "        model.learn(total_timesteps=10000)\n",
    "\n",
    "    val_env = create_validation_env(run.config.n_ticks)\n",
    "    state = val_env.reset()\n",
    "    portfolio_values = []\n",
    "    prices = []\n",
    "    actions = []\n",
    "\n",
    "    for i in range(len(val_data)):\n",
    "        action, _ = model.predict(state)\n",
    "\n",
    "        price, portfolio_value = env.get_data()\n",
    "        actions.append(action)\n",
    "        prices.append(price)\n",
    "        portfolio_values.append(portfolio_value)\n",
    "\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        wandb.log({'portfolio_value': portfolio_value})\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function=train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads3.6",
   "language": "python",
   "name": "ads3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
